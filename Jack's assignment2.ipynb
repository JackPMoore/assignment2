{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question.\n",
        "`! git clone https://www.github.com/DS3001/assignment2`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://www.github.com/DS3001/assignment2"
      ],
      "metadata": {
        "id": "BPbJ3M8FvuT_",
        "outputId": "b13f27dd-ebd6-465b-af18-6d90946d5a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BPbJ3M8FvuT_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignment2'...\n",
            "warning: redirecting to https://github.com/DS3001/assignment2.git/\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 36 (delta 9), reused 29 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (36/36), 5.47 MiB | 28.56 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uaHG5BiPv5qM"
      },
      "id": "uaHG5BiPv5qM",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.1**"
      ],
      "metadata": {
        "id": "eeHGxxOZ32ux"
      },
      "id": "eeHGxxOZ32ux"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This paper addresses the often-overlooked aspect of data cleaning known as \"data tidying.\" It emphasizes the importance of structuring data into tidy datasets, where each variable corresponds to a column, each observation to a row, and each type of observational unit forms a separate table. Tidy datasets facilitate easier data manipulation, modeling, and visualization, and this framework simplifies dealing with messy datasets. Additionally, it highlights how adhering to this structured approach simplifies the development of tools for data analysis, both inputting and outputting tidy datasets. The paper illustrates the benefits of this approach through a case study, showcasing the efficiency gains in data analysis.*"
      ],
      "metadata": {
        "id": "dGPSrUEW37AF"
      },
      "id": "dGPSrUEW37AF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.2**"
      ],
      "metadata": {
        "id": "YEd8-dfq3_jQ"
      },
      "id": "YEd8-dfq3_jQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The \"tidy data standard\" offers a structured and standardized framework for data organization, primarily aimed at streamlining and improving the data analysis process. It ensures consistency, reducing the need to start from scratch with each new dataset. Tidy data simplifies initial exploration, making data patterns and insights more accessible without extensive data wrangling. Moreover, it supports interoperability with \"tidy tools\" that handle both input and output of tidy datasets, reducing manual data translation between tools. This framework employs a user-friendly language, particularly beneficial for statisticians, aligning data structuring and analysis with their expertise. Ultimately, it empowers data analysts to focus on substantive analysis, minimizing the need for extensive data preparation and cleaning.*"
      ],
      "metadata": {
        "id": "Pd8gNtTL4Dsd"
      },
      "id": "Pd8gNtTL4Dsd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.3**"
      ],
      "metadata": {
        "id": "UCWHnUmW8ehl"
      },
      "id": "UCWHnUmW8ehl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In the context of data, it means that tidy datasets have a standardized and uniform structure, making them relatively similar and easy to work with. In contrast, messy datasets come in various disorganized forms, each with its unique challenges, making them difficult to handle.*\n",
        "\n",
        "*The next sentence suggests that when you have a specific dataset in front of you, it's relatively straightforward to identify what constitutes an observation (individual data points or rows) and what constitutes a variable (attributes or columns). However, defining these terms in a general or abstract sense can be challenging. While it's clear within the context of a single dataset, creating a universally applicable definition that covers all datasets is a more complex task due to the diversity of data types, structures, and analytical needs in the field of data analysis.*"
      ],
      "metadata": {
        "id": "8uxWQaHp8ijr"
      },
      "id": "8uxWQaHp8ijr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.4**"
      ],
      "metadata": {
        "id": "VbMPh0lL-3TU"
      },
      "id": "VbMPh0lL-3TU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Values: Values are individual data points or measurements that are typically either numbers (quantitative) or strings (qualitative). In the context of your passage, values can represent specific information, such as \"John\" as a value for the variable \"person\" or \"16\" as a value for the variable \"result.\" Values are the most granular level of data in a dataset.*\n",
        "\n",
        "*Variables: Variables are attributes or characteristics that group and organize values that measure the same underlying concept or property across different units or observations. In your description, there are three variables: \"person,\" \"treatment,\" and \"result.\" \"Person\" has three possible values (John, Mary, Jane), \"treatment\" has two possible values (a and b), and \"result\" has five or six values depending on how you handle the missing value.*\n",
        "\n",
        "*Observations: Observations represent the individual units or entities on which data is collected. Each observation contains values for all variables. In your passage, you have a completely crossed experimental design where every combination of \"person\" and \"treatment\" was measured, resulting in six observations. An observation can be thought of as a specific person receiving a specific treatment and yielding a specific result.*"
      ],
      "metadata": {
        "id": "sYxmK7FM_B6A"
      },
      "id": "sYxmK7FM_B6A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.5**"
      ],
      "metadata": {
        "id": "CrcWClxe64oY"
      },
      "id": "CrcWClxe64oY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tidy data is defined as a standardized way of organizing a dataset in which each variable corresponds to a column, each observation corresponds to a row, and each type of observational unit corresponds to a table, facilitating straightforward data analysis and extraction of variables.*"
      ],
      "metadata": {
        "id": "qMHaLPTd67o8"
      },
      "id": "qMHaLPTd67o8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.6**"
      ],
      "metadata": {
        "id": "aUL1Bvx87lFx"
      },
      "id": "aUL1Bvx87lFx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The five most common problems with messy datasets, as outlined in the passage, are as follows:*\n",
        "\n",
        "*1. Column headers are values, not variable names: In this case, the data contains information in the column headers that should be part of the actual data, making it difficult to analyze.*\n",
        "\n",
        "*2. Multiple variables are stored in one column: Data may be structured in a way where multiple variables are combined within a single column, making it challenging to extract and analyze these variables separately.*\n",
        "\n",
        "*3. Variables are stored in both rows and columns: Variables may be scattered across both rows and columns, making it hard to distinguish between variables and observations.*\n",
        "\n",
        "*4. Multiple types of observational units are stored in the same table: A single table may contain data from different types of observational units, leading to a lack of clear separation between them.*\n",
        "\n",
        "*5. A single observational unit is stored in multiple tables: Data related to a single observational unit is divided into multiple tables, making it difficult to analyze the data holistically.*\n",
        "\n",
        "*The data in Table 4 is messy because it violates the principles of tidy data. In this case, the issue is that multiple variables are stored in one column. The \"table\" column contains both the table names and the values that should be separate variables. This makes it challenging to work with the data in a straightforward manner.*\n",
        "\n",
        "**\"Melt\" or \"melting\" a dataset refers to the process of reshaping it from a wide format (where variables are spread across columns) to a long format (where variables are stacked in a single column). This is typically done to make the data tidier and more amenable to analysis. Melting involves restructuring the data so that each row represents a unique observation, and each variable is stacked in a single column with a corresponding value column. This process helps resolve the issue of multiple variables stored in one column, making the data easier to work with.*"
      ],
      "metadata": {
        "id": "Z4oj0xal7z_x"
      },
      "id": "Z4oj0xal7z_x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.7**"
      ],
      "metadata": {
        "id": "vS4FSCdb_NYk"
      },
      "id": "vS4FSCdb_NYk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Table 11 is considered messy because it violates the principles of tidy data. The specific issues that make Table 11 messy include:*\n",
        "\n",
        "*1. Variables are stored in both rows and columns: In Table 11, weather data is spread across rows (tmin, tmax) and columns (day, d1–d31). The temperature values (tmin and tmax) are found in the rows, while the day-specific values (d1–d31) are in the columns. This makes it challenging to work with and analyze the data efficiently.*\n",
        "\n",
        "*2. Variable names are stored in a separate column (element): The element column in Table 11 stores the names of the variables (tmin and tmax) instead of having these variable names as individual columns. This makes the data structure less intuitive and violates the tidy data principle that each variable should form a column.*\n",
        "\n",
        "*In Table 12, the data is tidy and \"molten\" because:*\n",
        "\n",
        "*1. The data is \"melted\" because columns such as id, year, month, and element are identifier variables. Melting reshapes the data from a wide format (with variables in both rows and columns) into a long format, where each row represents a unique observation.*\n",
        "\n",
        "*2. In Table 12(a), the missing values are dropped, which is permissible because the number of days in each month is known. This makes the missing values implicit rather than explicit, simplifying the presentation of the data.*"
      ],
      "metadata": {
        "id": "rOXcou8w_QUu"
      },
      "id": "rOXcou8w_QUu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.8**"
      ],
      "metadata": {
        "id": "g_1yT0U9DV7x"
      },
      "id": "g_1yT0U9DV7x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The \"chicken-and-egg\" problem in focusing on tidy data arises from its interdependence with tools designed for it, potentially hindering exploration of alternative data structures or manipulation methods. Wickham envisions future work in data wrangling to bring both incremental and substantial improvements. Tidy data, while a valuable framework, is not considered the final solution. He anticipates others will build on this foundation for better data storage and manipulation tools. Wickham also hopes that fields like user-centered design and human-computer interaction can contribute insights into the cognitive aspects of data analysis. Alternative data formulations, such as handling multidimensional arrays, and adaptive tools are potential areas of exploration. Addressing various data cleaning tasks, beyond just tidying data, is an ongoing endeavor to simplify and streamline the data preparation process.*"
      ],
      "metadata": {
        "id": "10QLmiG5DZTE"
      },
      "id": "10QLmiG5DZTE"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.1**"
      ],
      "metadata": {
        "id": "PADDlBaOzfR8"
      },
      "id": "PADDlBaOzfR8"
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb = pd.read_csv(\"assignment2/data/airbnb_hw.csv\")\n",
        "#print(airbnb)\n",
        "#print(type(airbnb.Price))\n",
        "airbnb['Price'].str.replace(',', '')\n",
        "airbnb['Price'] = pd.to_numeric(airbnb['Price'], errors='coerce')\n",
        "\n",
        "\n",
        "airbnb['Price'+'_nan'] = airbnb['Price'].isnull()\n",
        "\n",
        "\n",
        "print('Total Missings: \\n', sum(airbnb['Price'+'_nan']),'\\n')\n"
      ],
      "metadata": {
        "id": "9LIh_jH0wyZh",
        "outputId": "67bd812d-4ae1-4866-ff4b-a23e47165510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9LIh_jH0wyZh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Missings: \n",
            " 181 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The problem with the 'Price' variable in the Airbnb dataset was that once the price went about $1000, a comma was entered. This resulted in the variable being a character instead of numeric. The first part of this code replaced all of the commas with nothing. The next part coerces the variable into numeric and then changes all of the missing variables into NaN. In total there are about 181 missing variables out of about 30,000 observations*"
      ],
      "metadata": {
        "id": "e0-aX9KNLVPE"
      },
      "id": "e0-aX9KNLVPE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.2**"
      ],
      "metadata": {
        "id": "XprvNr1FL9RD"
      },
      "id": "XprvNr1FL9RD"
    },
    {
      "cell_type": "code",
      "source": [
        "sharksdf = pd.read_csv(\"assignment2/data/sharks.csv\")"
      ],
      "metadata": {
        "id": "OLVerXedL-qR",
        "outputId": "434a88c3-3374-404a-94ce-d9f7c92b27ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OLVerXedL-qR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6fa00c00b71b>:1: DtypeWarning: Columns (10,17,18,19,20,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  sharksdf = pd.read_csv(\"assignment2/data/sharks.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sharksdf['Type'].unique(),'\\n')"
      ],
      "metadata": {
        "id": "YrZ_rJWzMKm3",
        "outputId": "1824a9a0-009d-4ea7-dd17-42105f246b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YrZ_rJWzMKm3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Unconfirmed'\n",
            " 'Unverified' 'Invalid' 'Under investigation' 'Boating' 'Sea Disaster' nan\n",
            " 'Boat' 'Boatomg'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sharksdf['Type'] = sharksdf['Type'].replace('Watercraft', 'Watercraft/Boat Accident')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Unverified', 'Unconfirmed')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Questionable', 'Unconfirmed')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Invalid', 'Unconfirmed')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Boating', 'Watercraft/Boat Accident')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Sea Disaster', 'Watercraft/Boat Accident')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Boat', 'Watercraft/Boat Accident')\n",
        "sharksdf['Type'] = sharksdf['Type'].replace('Boatomg', 'Watercraft/Boat Accident')"
      ],
      "metadata": {
        "id": "4SarE_J9MiHl"
      },
      "id": "4SarE_J9MiHl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sharksdf['Type'].unique(),'\\n')"
      ],
      "metadata": {
        "id": "meSNKUcnNgIN",
        "outputId": "248b2d4f-cd92-4175-fe79-ef2707ab207f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "meSNKUcnNgIN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unprovoked' 'Provoked' 'Unconfirmed' 'Watercraft/Boat Accident'\n",
            " 'Under investigation' nan] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*For this section, there were many observations that were either misspelled or that did not apply. Many of them were also very similar. For the ones that were similar like watercraft, boating, boat, and sea disaster I combined them all into one variable. I also combined the unverified, questionable, and invalid observations into one variable as well. This way the observations that have the most similarities are combined into one type.*"
      ],
      "metadata": {
        "id": "RLC5xt8E2uu9"
      },
      "id": "RLC5xt8E2uu9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.3**"
      ],
      "metadata": {
        "id": "4ANMDEOE3YcA"
      },
      "id": "4ANMDEOE3YcA"
    },
    {
      "cell_type": "code",
      "source": [
        "trial = pd.read_csv(\"VirginiaPretrialData2017.csv\")"
      ],
      "metadata": {
        "id": "Z0Bf6HECd6v9",
        "outputId": "14976f79-04f6-49b9-9064-a8c93130d238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z0Bf6HECd6v9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-d09c0302fe04>:1: DtypeWarning: Columns (1,4,5,7,79,80,81,82,83,84,108,163,164,165,166,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,270,271,272,273,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,301,302,303,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,706) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  trial = pd.read_csv(\"VirginiaPretrialData2017.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trial['WhetherDefendantWasReleasedPretrial'].unique(),'\\n')"
      ],
      "metadata": {
        "id": "Xwx-vaIWu2NJ",
        "outputId": "cb949ae2-a37b-47c6-9f26-d6dfab7c9436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Xwx-vaIWu2NJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 0 1] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial['WhetherDefendantWasReleasedPretrial'] = trial['WhetherDefendantWasReleasedPretrial'].replace(9, '') #replaces the 9's with blanks\n",
        "\n",
        "trial['WhetherDefendantWasReleasedPretrial'] = trial['WhetherDefendantWasReleasedPretrial'].replace('',np.nan) #replaces the blanks with missing values\n",
        "\n",
        "\n",
        "trial['WhetherDefendantWasReleasedPretrial'+'_nan'] = trial['WhetherDefendantWasReleasedPretrial'].isnull() #creates a variable for the missing values\n",
        "\n",
        "print('Total Missings: \\n', sum(trial['WhetherDefendantWasReleasedPretrial'+'_nan']),'\\n')"
      ],
      "metadata": {
        "id": "59O12Z-I2c0p",
        "outputId": "1134d56d-03a5-4dcd-beb0-eee159f35a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "59O12Z-I2c0p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Missings: \n",
            " 31 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trial['WhetherDefendantWasReleasedPretrial'].unique(),'\\n')"
      ],
      "metadata": {
        "id": "Lx4unYt93D4R",
        "outputId": "dddae369-a997-403c-d361-a235c88dfc47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Lx4unYt93D4R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nan  0.  1.] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*For this question, I used the 'replace' function along with the np.nan to impute a missing value to all values that were listed as a 9. Then adding all of them, we see that there are 31 missing values for this variable*"
      ],
      "metadata": {
        "id": "CJ9uyeI9bAMn"
      },
      "id": "CJ9uyeI9bAMn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.4**"
      ],
      "metadata": {
        "id": "vxr5eNS7bo1t"
      },
      "id": "vxr5eNS7bo1t"
    },
    {
      "cell_type": "code",
      "source": [
        "print(trial['ImposedSentenceAllChargeInContactEvent'].unique(),'\\n')"
      ],
      "metadata": {
        "id": "VwChNmtIbrw7",
        "outputId": "312777c0-564a-42b1-9cc6-09700176a5dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VwChNmtIbrw7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ' '60' '12' '.985626283367556' '36' '6' '24' '5.91375770020534' '120'\n",
            " '72' '11.9917864476386' '0' '2.95687885010267' '84' '108' '300' '240'\n",
            " '180' '4' '96' '2' '54' '.328542094455852' '44' '5' '115' '132' '48'\n",
            " '258' '34' '76' '.164271047227926' '.131416837782341' '111' '9' '3'\n",
            " '1.97125256673511' '36.9856262833676' '.0657084188911704'\n",
            " '35.4928131416838' '106.492813141684' '8' '35' '18.3141683778234' '480'\n",
            " '32' '93' '234' '732' '1.16427104722793' '4.6570841889117' '21' '7'\n",
            " '4.49281314168378' '18' '600' '43.1642710472279' '179' '52' '30' '20'\n",
            " '192' '702' '14' '55' '53' '11.9055441478439' '114' '35.0061601642711'\n",
            " '68' '.657084188911704' '46.6242299794661' '102' '65' '200' '57'\n",
            " '24.3285420944559' '12.1642710472279' '117' '81.4928131416838'\n",
            " '22.4928131416838' '1980' '3.6570841889117' '56' '10' '2.79260780287474'\n",
            " '1' '47' '22' '1500' '40' '284' '11' '118' '42' '162' '156'\n",
            " '47.2956878850103' '105' '51' '246' '29' '75' '324' '360'\n",
            " '34.4804928131417' '120.328542094456' '59.9260780287474' '66'\n",
            " '59.9917864476386' '660' '51.1642710472279' '14.9568788501027'\n",
            " '3.98562628336756' '78' '228' '1.47843942505133' '62' '4.8' '86' '168'\n",
            " '23' '33' '48.0328542094456' '720' '348' '1200' '27' '49' '87' '420' '63'\n",
            " '79.9260780287474' '57.0349075975359' '49.9712525667351'\n",
            " '59.4928131416838' '17' '238.492813141684' '60.9856262833676' '126' '45'\n",
            " '158' '216' '227' '42.9568788501027' '445' '70.952772073922' '516'\n",
            " '177.82135523614' '1752' '90' '1080' '141' '4.82956878850103' '230' '31'\n",
            " '2208' '52.5133470225873' '69' '26' '33.4928131416838' '140' '131' '344'\n",
            " '219' '101' '71' '59' '58' '120.197125256674' '67' '35.4004106776181'\n",
            " '3.28542094455852' '40.1642710472279' '91' '1.7741273100616' '155'\n",
            " '34.4928131416838' '81' '92.3285420944559' '3.5482546201232' '207' '74'\n",
            " '518' '28' '8.95687885010267' '237' '404.673511293634' '18.1642710472279'\n",
            " '10.7433264887064' '551' '39' '15' '124' '43' '176' '19.4928131416838'\n",
            " '482' '129' '88' '46' '45.8542094455852' '128.628336755647'\n",
            " '136.492813141684' '108.328542094456' '50' '363.663244353183' '288' '250'\n",
            " '107' '81.0225872689938' '444' '205' '10.6570841889117' '19'\n",
            " '66.9856262833676' '38.4928131416838' '264' '276' '173' '222' '144' '294'\n",
            " '336' '431' '450' '73' '99.3285420944559' '128' '30.8069815195072'\n",
            " '31.5256673511294' '127' '202' '55.3285420944559' '89' '242'\n",
            " '1.31416837782341' '1029' '.788501026694045' '194.858316221766' '399'\n",
            " '39.6570841889117' '56.95687885' '198' '120.985626283368'\n",
            " '47.6570841889117' '148' '6.8993839835729' '65.3285420944559'\n",
            " '5.95277207392197' '.0985626283367557' '3.32854209445585'\n",
            " '3.94250513347023' '12.9856262833676' '6.98562628336756'\n",
            " '13.1498973305955' '15.1642710472279' '17.1971252566735'\n",
            " '17.9137577002053' '104' '212' '24.6570841889117' '72.6570841889117'\n",
            " '2.98562628336756' '144.985626283368' '31.9712525667351' '183'\n",
            " '4.98562628336756' '11.8213552361396' '252' '12.394250513347'\n",
            " '42.4928131416838' '10.1642710472279' '11.1642710472279'\n",
            " '5.49281314168378' '59.6632443531827' '12.3285420944559'\n",
            " '48.9856262833676' '240.985626283368' '2.6570841889117' '540'\n",
            " '2.97125256673511' '6.32854209445585' '23.6632443531828'\n",
            " '133.657084188912' '35.3285420944559' '456' '103' '1.72279260780287'\n",
            " '12.6570841889117' '11.6570841889117' '60.3285420944559'\n",
            " '3.78850102669405' '576' '2.13141683778234' '492' '14.9856262833676'\n",
            " '24.9856262833676' '61.9712525667351' '5.6570841889117' '16'\n",
            " '42.1642710472279' '.492813141683778' '138' '13.3141683778234'\n",
            " '11.8932238193018' '5.32854209445585' '95' '62.6570841889117'\n",
            " '3.08829568788501' '11.8275154004107' '1.64271047227926'\n",
            " '47.9917864476386' '4.27104722792608' '8.32854209445585'\n",
            " '3.31416837782341' '70' '77' '1.09856262833676' '48.1642710472279'\n",
            " '27.4928131416838' '6.93839835728953' '1011' '.68993839835729'\n",
            " '1.1170431211499' '1.49281314168378' '4.16427104722793'\n",
            " '1.19712525667351' '4.07392197125257' '188' '11.3285420944559'\n",
            " '.0328542094455852' '432' '11.952772073922' '36.4928131416838'\n",
            " '23.9835728952772' '9.98562628336756' '98' '36.3285420944559' '112'\n",
            " '.394250513347023' '13' '.262833675564682' '13.7987679671458'\n",
            " '5.8870636550308' '354' '5.91991786447639' '24.1642710472279'\n",
            " '62.95687885' '4.59958932238193' '123' '2.32854209445585'\n",
            " '23.9240246406571' '204' '197' '174' '16.1498973305955' '840' '440'\n",
            " '98.95687885' '17.952772073922' '63.9425051334702' '60.1314168377823'\n",
            " '12.1314168377823' '172.952772073922' '.197125256673511'\n",
            " '138.164271047228' '4.92813141683778' '.919917864476386'\n",
            " '18.9856262833676' '6.6570841889117' '2.85420944558522'\n",
            " '8.91375770020534' '146' '12.4928131416838' '.558521560574949'\n",
            " '.722792607802875' '5.82135523613963' '84.9856262833676'\n",
            " '6.16427104722793' '15.9856262833676' '64.5585215605749'\n",
            " '38.299794661191' '11.958932238193' '3.1211498973306' '126.328542094456'\n",
            " '5.16427104722793' '64' '42.6570841889117' '312' '19.9712525667351'\n",
            " '82.3285420944559' '23.9712525667351' '17.6242299794661'\n",
            " '121.971252566735' '59.6550308008214' '1.32854209445585'\n",
            " '7.97125256673511' '1.91991786447639' '.525667351129363'\n",
            " '9.32854209445585' '42.9856262833676' '41.9137577002053'\n",
            " '72.9856262833676' '12.4784394250513' '5.19096509240246' '473'\n",
            " '16.6570841889117' '109' '86.3285420944559' '41' '1.90554414784394'\n",
            " '94.1642710472279' '302' '4.39425051334702' '10.8213552361396'\n",
            " '18.3285420944559' '154' '83' '110.956878850103' '226' '96.0328542094456'\n",
            " '4.82135523613963' '30.3285420944559' '37.9712525667351'\n",
            " '50.4640657084189' '286' '99' '99.4928131416838' '2.6611909650924'\n",
            " '70.9712525667351' '13.9712525667351' '23.6570841889117'\n",
            " '.459958932238193' '132.492813141684' '283' '49.3141683778234'\n",
            " '27.9856262833676' '38' '7.6570841889117' '83.6550308008214'\n",
            " '10.9199178644764' '162.328542094456' '37' '132.328542094456'\n",
            " '35.952772073922' '165' '10.9856262833676' '20.1642710472279'\n",
            " '2.59137577002053' '175' '180.985626283368' '10.3285420944559'\n",
            " '36.1642710472279' '120.657084188912' '232' '152' '8.98562628336756'\n",
            " '167' '11.0657084188912' '11.2032854209446' '5.19712525667351'\n",
            " '3.16427104722793' '60.1642710472279' '1.18275154004107'\n",
            " '21.1642710472279' '2.19712525667351' '4.19712525667351'\n",
            " '2.62833675564682' '119.952772073922' '119.958932238193'\n",
            " '9.49281314168378' '5.25667351129363' '15.3285420944559'\n",
            " '2.82135523613963' '192.985626283368' '48.6570841889117'\n",
            " '5.95687885010267' '2.29979466119097' '960' '2.36550308008214' '116'\n",
            " '19.5133470225873' '1.6570841889117'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial['ImposedSentenceAllChargeInContactEvent'] = pd.to_numeric(trial['ImposedSentenceAllChargeInContactEvent'], errors='coerce')"
      ],
      "metadata": {
        "id": "uJvZrgIDhB0O"
      },
      "id": "uJvZrgIDhB0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial['ImposedSentenceAllChargeInContactEvent'] = trial['ImposedSentenceAllChargeInContactEvent'].replace('',np.nan)"
      ],
      "metadata": {
        "id": "LQGuhe7vhepc"
      },
      "id": "LQGuhe7vhepc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial['ImposedSentenceAllChargeInContactEvent'] = trial['ImposedSentenceAllChargeInContactEvent'] * 30.437"
      ],
      "metadata": {
        "id": "EnSdD9_chxzc"
      },
      "id": "EnSdD9_chxzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial['ImposedSentenceAllChargeInContactEvent'] = round(trial['ImposedSentenceAllChargeInContactEvent'])"
      ],
      "metadata": {
        "id": "_p2hyJP7iQYI"
      },
      "id": "_p2hyJP7iQYI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*For this question, the variable was given in terms of months. Each month has a different number of days so that can lead to many and long decimals for each observation. To start, I made sure everything was numeric and then replaced all the blank values with np.nan's. From there I mulitplied the variable by 30.437 which is the average number of days per month. After that, I rounded the variable to the nearest whole number. The variable is now expressed in terms of whole days instead of months with long decimal places.*"
      ],
      "metadata": {
        "id": "o2Z3jEyIirMo"
      },
      "id": "o2Z3jEyIirMo"
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q5.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.1**\n",
        "\n",
        "*The most recent US Census gathered data on race through a self-identification question, where individuals were asked to select their race or races from predefined categories, which included options like White, Black or African American, Asian, and more.*"
      ],
      "metadata": {
        "id": "JWUle5sRNhMN"
      },
      "id": "JWUle5sRNhMN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.2**\n",
        "\n",
        "*Gathering demographic data, including data on race, serves several important purposes:*\n",
        "\n",
        "1. *Resource Allocation: Data informs the distribution of government resources, like funding for schools, healthcare, and infrastructure, to address the needs of different communities.*\n",
        "\n",
        "2. *Political Representation: Data determines the allocation of congressional seats, influencing political power and representation.*\n",
        "\n",
        "3. *Civil Rights Enforcement: Data helps identify disparities and discrimination, aiding in the enforcement of civil rights laws.*\n",
        "\n",
        "*Data quality is crucial because inaccurate data can lead to misallocation of resources, unequal representation, and hinder efforts to address social inequalities and disparities. Accurate data is essential for fair policy-making and social progress.*"
      ],
      "metadata": {
        "id": "WvYkvR5SPrZZ"
      },
      "id": "WvYkvR5SPrZZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.3**\n",
        "\n",
        "*The 2020 Census had several strengths, such as the introduction of online response, which was a significant step towards modernization. This made it more convenient for many individuals to participate and potentially reduced costs associated with paper-based surveys. The Census Bureau undertook extensive outreach and public awareness campaigns to encourage participation, increasing the overall response rate.*\n",
        "\n",
        "*However, there were areas where the 2020 Census could have been improved.*\n",
        "\n",
        "1. *Hard-to-Reach Populations: Despite the outreach efforts, certain hard-to-reach populations, like immigrants, minorities, and low-income individuals, may still be undercounted due to language barriers, mistrust, or fear. Future surveys should develop more targeted outreach strategies and provide resources to facilitate participation within these communities.*\n",
        "\n",
        "2. *Digital Divide: While online response was a positive step, the digital divide remains a concern. Some people lack internet access or digital literacy, which could hinder their participation. Future surveys should ensure that all segments of the population have equal access to participation methods.*\n",
        "\n",
        "3. *Data Privacy and Security: In an era of heightened concern about data privacy and security, the Census Bureau must continuously work to assure the public that their information is protected and their privacy respected.*"
      ],
      "metadata": {
        "id": "9jn5b9kMRN57"
      },
      "id": "9jn5b9kMRN57"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.4**\n",
        "\n",
        "*The 2020 Census collected data on sex through a binary male/female question, which has been the traditional approach. However, the Census did not collect specific data on gender identity, which is a significant limitation. Constructive criticism of this practice includes:*\n",
        "\n",
        "1. *Lack of Inclusivity: The binary male/female question does not reflect the full spectrum of gender identities. It overlooks non-binary, genderqueer, transgender, and other gender-diverse individuals. Future surveys should include more inclusive gender options to respect and acknowledge the diversity of the population.*\n",
        "\n",
        "2. *Data Relevance: Accurate and inclusive data on gender identity is crucial for policy-making, healthcare planning, and understanding disparities experienced by gender-diverse individuals. Without this data, it's challenging to address the unique needs of these communities effectively.*\n",
        "\n",
        "3. *Data Privacy and Safety: It's essential to consider the privacy and safety of respondents when collecting gender identity data, as some individuals may fear discrimination or harm. Future practices should ensure anonymity and secure handling of this sensitive information.*"
      ],
      "metadata": {
        "id": "tKPO1YKcRkRz"
      },
      "id": "tKPO1YKcRkRz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.5**\n",
        "\n",
        "*When cleaning data, especially data related to protected characteristics like sex, gender, sexual identity, or race, several concerns must be addressed. Firstly, there is a significant concern about data privacy and the potential for re-identification, as even anonymized data can sometimes be used to infer sensitive information. Ensuring robust data anonymization techniques and adherence to data protection regulations is crucial.\n",
        "Secondly, when dealing with protected characteristics, missing values can be a challenge. The absence of data on these characteristics can lead to biased analysis and decision-making. Some common bad practices include ignoring missing values, which can perpetuate inequalities or misunderstand the true impact of policies on marginalized groups. Imputing missing data without considering the potential bias introduced is another concern, as it can lead to inaccurate results.\n",
        "On the other hand, good practices involve careful handling of missing data, such as transparently reporting the extent of missing values and exploring the reasons behind these gaps. Collecting more comprehensive and inclusive data from the outset can also mitigate issues related to missing values. Additionally, conducting sensitivity analyses to assess the potential impact of missing data on results is a valuable practice. The key is to be aware of the unique challenges associated with protected characteristics and to approach data cleaning with a commitment to fairness, accuracy, and privacy.*"
      ],
      "metadata": {
        "id": "4Uto4ZmnSIWL"
      },
      "id": "4Uto4ZmnSIWL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.6**\n",
        "\n",
        "*Imputing protected characteristics like race, gender, sex, or sexuality through algorithms raises ethical concerns regarding individual autonomy and privacy, potentially perpetuating stereotypes and biases. It risks infringing upon the right to self-identify. Ensuring fairness and accuracy is vital, as algorithmic biases could exacerbate societal prejudices, impacting decision-making and policy formulation. The fluid nature of these characteristics and the quality of initial data further complicate the accuracy of imputed values. Additionally, legal and regulatory compliance, particularly in the context of privacy laws, must be addressed when handling such sensitive data. Imputing these characteristics should be approached with extreme caution, with a focus on transparency, consent, privacy protection, and the prevention of biases, while adhering to relevant legal mandates.*"
      ],
      "metadata": {
        "id": "PUzhNmK2SrMw"
      },
      "id": "PUzhNmK2SrMw"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}